{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159bfb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00813bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "            PREFIX : <http://rdf.freebase.com/ns/>\n",
      "            SELECT distinct ?x0 as ?r0 ?y as ?r1 WHERE {\n",
      "            ?x1 ?x0 :m.0b787yg. \n",
      "                ?x2 ?y ?x1 .\n",
      "                  FILTER regex(?x0, \"http://rdf.freebase.com/ns/\")\n",
      "                  FILTER regex(?y, \"http://rdf.freebase.com/ns/\")\n",
      "                  }\n",
      "                  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/data/tianyuhang/KBQA/KB-BINDER/sparql_exe.py:812\u001b[0m, in \u001b[0;36mget_2hop_relations\u001b[0;34m(entity)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msparql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert()\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError:\n",
      "File \u001b[0;32m/data/tianyuhang/.conda/envs/binder/lib/python3.8/site-packages/SPARQLWrapper/Wrapper.py:1107\u001b[0m, in \u001b[0;36mSPARQLWrapper.query\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;124;03m    Execute the query.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;124;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;124;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/data/tianyuhang/.conda/envs/binder/lib/python3.8/site-packages/SPARQLWrapper/Wrapper.py:1073\u001b[0m, in \u001b[0;36mSPARQLWrapper._query\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturnFormat\n",
      "File \u001b[0;32m/data/tianyuhang/.conda/envs/binder/lib/python3.8/urllib/request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/tianyuhang/.conda/envs/binder/lib/python3.8/urllib/request.py:522\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 522\u001b[0m     req \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n",
      "File \u001b[0;32m/data/tianyuhang/.conda/envs/binder/lib/python3.8/urllib/request.py:1241\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_request_\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno host given\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# POST\u001b[39;00m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error no host given>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m relas \u001b[38;5;241m=\u001b[39m \u001b[43mget_2hop_relations\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mm.0b787yg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/tianyuhang/KBQA/KB-BINDER/sparql_exe.py:815\u001b[0m, in \u001b[0;36mget_2hop_relations\u001b[0;34m(entity)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(query1)\n\u001b[0;32m--> 815\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbindings\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    817\u001b[0m     r1 \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr1\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://rdf.freebase.com/ns/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "relas = get_2hop_relations(\"m.0b787yg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7aad772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import spacy\n",
    "from sparql_exe import execute_query, get_types, get_2hop_relations, lisp_to_sparql\n",
    "from utils import process_file, process_file_node, process_file_rela, process_file_test\n",
    "from rank_bm25 import BM25Okapi\n",
    "from time import sleep\n",
    "import re\n",
    "import logging\n",
    "from collections import Counter\n",
    "import argparse\n",
    "from pyserini.search import FaissSearcher, LuceneSearcher\n",
    "from pyserini.search.hybrid import HybridSearcher\n",
    "from pyserini.search.faiss import AutoQueryEncoder\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "                    level=logging.INFO,\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger(\"time recoder\")\n",
    "\n",
    "def select_shot_prompt_train(train_data_in, shot_number):\n",
    "    random.shuffle(train_data_in)\n",
    "    compare_list = [\"le\", \"ge\", \"gt\", \"lt\", \"ARGMIN\", \"ARGMAX\"]\n",
    "    if shot_number == 1:\n",
    "        selected_quest_compose = [train_data_in[0][\"question\"]]\n",
    "        selected_quest_compare = [train_data_in[0][\"question\"]]\n",
    "        selected_quest = [train_data_in[0][\"question\"]]\n",
    "    else:\n",
    "        selected_quest_compose = []\n",
    "        selected_quest_compare = []\n",
    "        each_type_num = shot_number // 2\n",
    "        for data in train_data_in:\n",
    "            if any([x in data['s_expression'] for x in compare_list]):\n",
    "                selected_quest_compare.append(data[\"question\"])\n",
    "                if len(selected_quest_compare) == each_type_num:\n",
    "                    break\n",
    "        for data in train_data_in:\n",
    "            if not any([x in data['s_expression'] for x in compare_list]):\n",
    "                selected_quest_compose.append(data[\"question\"])\n",
    "                if len(selected_quest_compose) == each_type_num:\n",
    "                    break\n",
    "        mix_type_num = each_type_num // 3\n",
    "        selected_quest = selected_quest_compose[:mix_type_num] + selected_quest_compare[:mix_type_num]\n",
    "    logger.info(\"selected_quest_compose: {}\".format(selected_quest_compose))\n",
    "    logger.info(\"selected_quest_compare: {}\".format(selected_quest_compare))\n",
    "    logger.info(\"selected_quest: {}\".format(selected_quest))\n",
    "    return selected_quest_compose, selected_quest_compare, selected_quest\n",
    "\n",
    "def sub_mid_to_fn(question, string, question_to_mid_dict):\n",
    "    seg_list = string.split()\n",
    "    mid_to_start_idx_dict = {}\n",
    "    for seg in seg_list:\n",
    "        if seg.startswith(\"m.\") or seg.startswith(\"g.\"):\n",
    "            mid = seg.strip(')(')\n",
    "            start_index = string.index(mid)\n",
    "            mid_to_start_idx_dict[mid] = start_index\n",
    "    if len(mid_to_start_idx_dict) == 0:\n",
    "        return string\n",
    "    start_index = 0\n",
    "    new_string = ''\n",
    "    for key in mid_to_start_idx_dict:\n",
    "        b_idx = mid_to_start_idx_dict[key]\n",
    "        e_idx = b_idx + len(key)\n",
    "        new_string = new_string + string[start_index:b_idx] + question_to_mid_dict[question][key]\n",
    "        start_index = e_idx\n",
    "    new_string = new_string + string[start_index:]\n",
    "    return new_string\n",
    "\n",
    "\n",
    "def type_generator(question, prompt_type, api_key, LLM_engine):\n",
    "    sleep(1)\n",
    "    prompt = prompt_type\n",
    "    prompt = prompt + \" Question: \" + question + \"Type of the question: \"\n",
    "    #print(prompt)\n",
    "    got_result = False\n",
    "    while got_result != True:\n",
    "        try:\n",
    "            openai.api_key = \"none\"\n",
    "            openai.api_base = \"http://127.0.0.1:8008/v1\"\n",
    "            answer_modi = openai.ChatCompletion.create(\n",
    "                    model='Qwen-14B-Chat',\n",
    "                    messages=[\n",
    "                        {\"role\":\"user\",\"content\":prompt}\n",
    "                        ],\n",
    "                    stream=False,\n",
    "                    stop=[]\n",
    "                    )\n",
    "#             print(answer_modi)\n",
    "            '''\n",
    "            answer_modi = openai.Completion.create(\n",
    "                engine=LLM_engine,\n",
    "                prompt=prompt,\n",
    "                temperature=0,\n",
    "                max_tokens=256,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stop=[\"Question: \"]\n",
    "            )\n",
    "            '''\n",
    "            got_result = True\n",
    "        except:\n",
    "            sleep(3)\n",
    "    gene_exp = answer_modi.choices[0].message.content\n",
    "    return gene_exp\n",
    "\n",
    "\n",
    "def ep_generator(question, selected_examples, temp, que_to_s_dict_train, question_to_mid_dict, api_key, LLM_engine,\n",
    "                 retrieval=False, corpus=None, nlp_model=None, bm25_train_full=None, retrieve_number=100):\n",
    "    if retrieval:\n",
    "        tokenized_query = nlp_model(question)\n",
    "        tokenized_query = [token.lemma_ for token in tokenized_query]\n",
    "        top_ques = bm25_train_full.get_top_n(tokenized_query, corpus, n=retrieve_number)\n",
    "        doc_scores = bm25_train_full.get_scores(tokenized_query)\n",
    "        top_score = max(doc_scores)\n",
    "        logger.info(\"top_score: {}\".format(top_score))\n",
    "        logger.info(\"top related questions: {}\".format(top_ques))\n",
    "        selected_examples = top_ques\n",
    "    prompt = \"\"\n",
    "    for que in selected_examples:\n",
    "        if not que_to_s_dict_train[que]:\n",
    "            continue\n",
    "        prompt = prompt + \"Question: \" + que + \"\\n\" + \"Logical Form: \" + sub_mid_to_fn(que, que_to_s_dict_train[que], question_to_mid_dict) + \"\\n\"\n",
    "    prompt = prompt + \"Question: \" + question + \"\\n\" + \"Logical Form: \"\n",
    "    got_result = False\n",
    "    #print(prompt)\n",
    "\n",
    "    while got_result != True:\n",
    "        try:\n",
    "            openai.api_key = \"none\"\n",
    "            openai.api_base = \"http://127.0.0.1:8008/v1\"\n",
    "            answer_modi = openai.ChatCompletion.create(\n",
    "                    model='Qwen-14B-Chat',\n",
    "                    messages=[\n",
    "                        {\"role\":\"user\",\"content\":prompt}\n",
    "                        ],\n",
    "                    stream=False,\n",
    "                    stop=[]\n",
    "                    )\n",
    "#             print(answer_modi)\n",
    "            '''\n",
    "            answer_modi = openai.Completion.create(\n",
    "                engine=LLM_engine,\n",
    "                prompt=prompt,\n",
    "                temperature=0,\n",
    "                max_tokens=256,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stop=[\"Question: \"]\n",
    "            )\n",
    "            '''\n",
    "            got_result = True\n",
    "        except:\n",
    "            sleep(3)\n",
    "    #gene_exp = [exp[\"text\"].strip() for exp in answer_modi[\"choices\"]]\n",
    "    gene_exp=answer_modi.choices[0].message.content\n",
    "#     print(gene_exp)\n",
    "    return gene_exp\n",
    "\n",
    "\n",
    "def convert_to_frame(s_exp):\n",
    "    phrase_set = [\"(JOIN\", \"(ARGMIN\", \"(ARGMAX\", \"(R\", \"(le\", \"(lt\", \"(ge\", \"(gt\", \"(COUNT\", \"(AND\", \"(TC\", \"(CONS\"]\n",
    "    seg_list = s_exp.split()\n",
    "    after_filter_list = []\n",
    "    for seg in seg_list:\n",
    "        for phrase in phrase_set:\n",
    "            if phrase in seg:\n",
    "                after_filter_list.append(phrase)\n",
    "        if \")\" in seg:\n",
    "            after_filter_list.append(''.join(i for i in seg if i == ')'))\n",
    "    return ''.join(after_filter_list)\n",
    "\n",
    "\n",
    "def find_friend_name(gene_exp, org_question):\n",
    "    seg_list = gene_exp.split()\n",
    "    phrase_set = [\"(JOIN\", \"(ARGMIN\", \"(ARGMAX\", \"(R\", \"(le\", \"(lt\", \"(ge\", \"(gt\", \"(COUNT\", \"(AND\"]\n",
    "    temp = []\n",
    "    reg_ents = []\n",
    "    for i, seg in enumerate(seg_list):\n",
    "        if not any([ph in seg for ph in phrase_set]):\n",
    "            if seg.lower() in org_question:\n",
    "                temp.append(seg.lower())\n",
    "            if seg.endswith(')'):\n",
    "                stripped = seg.strip(')')\n",
    "                stripped_add = stripped + ')'\n",
    "                if stripped_add.lower() in org_question:\n",
    "                    temp.append(stripped_add.lower())\n",
    "                    reg_ents.append(\" \".join(temp).lower())\n",
    "                    temp = []\n",
    "                elif stripped.lower() in org_question:\n",
    "                    temp.append(stripped.lower())\n",
    "                    reg_ents.append(\" \".join(temp).lower())\n",
    "                    temp = []\n",
    "    if len(temp) != 0:\n",
    "        reg_ents.append(\" \".join(temp))\n",
    "    return reg_ents\n",
    "\n",
    "def get_right_mid_set(fn, id_dict, question):\n",
    "    type_to_mid_dict = {}\n",
    "    type_list = []\n",
    "    for mid in id_dict:\n",
    "        types = get_types(mid)\n",
    "        for cur_type in types:\n",
    "            if not cur_type.startswith(\"common.\") and not cur_type.startswith(\"base.\"):\n",
    "                if cur_type not in type_to_mid_dict:\n",
    "                    type_to_mid_dict[cur_type] = {}\n",
    "                    type_to_mid_dict[cur_type][mid] = id_dict[mid]\n",
    "                else:\n",
    "                    type_to_mid_dict[cur_type][mid] = id_dict[mid]\n",
    "                type_list.append(cur_type)\n",
    "    tokenized_type_list = [re.split('\\.|_', doc) for doc in type_list]\n",
    "    #     tokenized_question = tokenizer.tokenize(question)\n",
    "    tokenized_question = question.split()\n",
    "    bm25 = BM25Okapi(tokenized_type_list)\n",
    "    top10_types = bm25.get_top_n(tokenized_question, type_list, n=10)\n",
    "    selected_types = top10_types[:3]\n",
    "    selected_mids = []\n",
    "    for any_type in selected_types:\n",
    "        # logger.info(\"any_type: {}\".format(any_type))\n",
    "        # logger.info(\"type_to_mid_dict[any_type]: {}\".format(type_to_mid_dict[any_type]))\n",
    "        selected_mids += list(type_to_mid_dict[any_type].keys())\n",
    "    return selected_mids\n",
    "\n",
    "def from_fn_to_id_set(fn_list, question, name_to_id_dict, bm25_all_fns, all_fns):\n",
    "    return_mid_list = []\n",
    "    for fn_org in fn_list:\n",
    "        drop_dot = fn_org.split()\n",
    "        drop_dot = [seg.strip('.') for seg in drop_dot]\n",
    "        drop_dot = \" \".join(drop_dot)\n",
    "        if fn_org.lower() not in question and drop_dot.lower() in question:\n",
    "            fn_org = drop_dot\n",
    "        if fn_org.lower() not in name_to_id_dict:\n",
    "            logger.info(\"fn_org: {}\".format(fn_org.lower()))\n",
    "            tokenized_query = fn_org.lower().split()\n",
    "            fn = bm25_all_fns.get_top_n(tokenized_query, all_fns, n=1)[0]\n",
    "            logger.info(\"sub fn: {}\".format(fn))\n",
    "        else:\n",
    "            fn = fn_org\n",
    "        if fn.lower() in name_to_id_dict:\n",
    "            id_dict = name_to_id_dict[fn.lower()]\n",
    "        if len(id_dict) > 15:\n",
    "            mids = get_right_mid_set(fn.lower(), id_dict, question)\n",
    "        else:\n",
    "            mids = sorted(id_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "            mids = [mid[0] for mid in mids]\n",
    "        return_mid_list.append(mids)\n",
    "    return return_mid_list\n",
    "\n",
    "\n",
    "\n",
    "def convz_fn_to_mids(gene_exp, found_names, found_mids):\n",
    "    if len(found_names) == 0:\n",
    "        return gene_exp\n",
    "    start_index = 0\n",
    "    new_string = ''\n",
    "    for name, mid in zip(found_names, found_mids):\n",
    "        b_idx = gene_exp.lower().index(name)\n",
    "        e_idx = b_idx + len(name)\n",
    "        new_string = new_string + gene_exp[start_index:b_idx] + mid\n",
    "        start_index = e_idx\n",
    "    new_string = new_string + gene_exp[start_index:]\n",
    "    return new_string\n",
    "\n",
    "def add_reverse(org_exp):\n",
    "    final_candi = [org_exp]\n",
    "    total_join = 0\n",
    "    list_seg = org_exp.split(\" \")\n",
    "    for seg in list_seg:\n",
    "        if \"JOIN\" in seg:\n",
    "            total_join += 1\n",
    "    for i in range(total_join):\n",
    "        final_candi = final_candi + add_reverse_index(final_candi, i + 1)\n",
    "    return final_candi\n",
    "\n",
    "\n",
    "def add_reverse_index(list_of_e, join_id):\n",
    "    added_list = []\n",
    "    list_of_e_copy = list_of_e.copy()\n",
    "    for exp in list_of_e_copy:\n",
    "        list_seg = exp.split(\" \")\n",
    "        count = 0\n",
    "        for i, seg in enumerate(list_seg):\n",
    "            if \"JOIN\" in seg and \".\" in list_seg[i + 1]:\n",
    "                count += 1\n",
    "                if count != join_id:\n",
    "                    continue\n",
    "                list_seg[i + 1] = \"(R \" + list_seg[i + 1] + \")\"\n",
    "                added_list.append(\" \".join(list_seg))\n",
    "                break\n",
    "            if \"JOIN\" in seg and \"(R\" in list_seg[i + 1]:\n",
    "                count += 1\n",
    "                if count != join_id:\n",
    "                    continue\n",
    "                list_seg[i + 1] = \"\"\n",
    "                list_seg[i + 2] = list_seg[i + 2][:-1]\n",
    "                added_list.append(\" \".join(\" \".join(list_seg).split()))\n",
    "                break\n",
    "    return added_list\n",
    "\n",
    "\n",
    "def bound_to_existed(question, s_expression, found_mids, two_hop_rela_dict,\n",
    "                     relationship_to_enti, hsearcher, rela_corpus, relationships):\n",
    "    possible_relationships_can = []\n",
    "    possible_relationships = []\n",
    "    logger.info(\"before 2 hop rela\")\n",
    "    updating_two_hop_rela_dict = two_hop_rela_dict.copy()\n",
    "    logger.info(\"mids is {}\".format(found_mids))\n",
    "    for mid in found_mids:\n",
    "        logger.info(\"mid is {}\".format(mid))\n",
    "        if mid in updating_two_hop_rela_dict:\n",
    "            logger.info(\"1\")\n",
    "            relas = updating_two_hop_rela_dict[mid]\n",
    "            possible_relationships_can += list(set(relas[0]))\n",
    "            possible_relationships_can += list(set(relas[1]))\n",
    "        else:\n",
    "            logger.info(\"2\")\n",
    "            try:\n",
    "                relas = get_2hop_relations(mid)\n",
    "            except Exception as e:\n",
    "                logger.info(\"Damn it!\")\n",
    "                logger.info(\"Exception is {}\".format(e))\n",
    "            logger.info(\"rela is {}\".format(relas))\n",
    "            updating_two_hop_rela_dict[mid] = relas\n",
    "            possible_relationships_can += list(set(relas[0]))\n",
    "            possible_relationships_can += list(set(relas[1]))\n",
    "    logger.info(\"after 2 hop rela\")\n",
    "    for rela in possible_relationships_can:\n",
    "        if not rela.startswith('common') and not rela.startswith('base') and not rela.startswith('type'):\n",
    "            possible_relationships.append(rela)\n",
    "    if not possible_relationships:\n",
    "        possible_relationships = relationships.copy()\n",
    "    expression_segment = s_expression.split(\" \")\n",
    "    print(\"possible_relationships: \", possible_relationships)\n",
    "    possible_relationships = list(set(possible_relationships))\n",
    "    relationship_replace_dict = {}\n",
    "    lemma_tags = {\"NNS\", \"NNPS\"}\n",
    "    for i, seg in enumerate(expression_segment):\n",
    "        processed_seg = seg.strip(')')\n",
    "        if '.' in seg and not seg.startswith('m.') and not seg.startswith('g.') and not (\n",
    "                expression_segment[i - 1].endswith(\"AND\") or expression_segment[i - 1].endswith(\"COUNT\") or\n",
    "                expression_segment[i - 1].endswith(\"MAX\") or expression_segment[i - 1].endswith(\"MIN\")) and (\n",
    "                not any(ele.isupper() for ele in seg)):\n",
    "            tokenized_query = re.split('\\.|_', processed_seg)\n",
    "            tokenized_query = \" \".join(tokenized_query)\n",
    "            tokenized_question = question.strip(' ?')\n",
    "            tokenized_query = tokenized_query + ' ' + tokenized_question\n",
    "            searched_results = hsearcher.search(tokenized_query, k=1000)\n",
    "            top3_ques = []\n",
    "            for hit in searched_results:\n",
    "                if len(top3_ques) > 7:\n",
    "                    break\n",
    "                cur_result = json.loads(rela_corpus.doc(str(hit.docid)).raw())\n",
    "                cur_rela = cur_result['rel_ori']\n",
    "                if not cur_rela.startswith(\"base.\") and not cur_rela.startswith(\"common.\") and \\\n",
    "                        not cur_rela.endswith(\"_inv.\") and len(cur_rela.split('.')) > 2 and \\\n",
    "                        cur_rela in possible_relationships:\n",
    "                    top3_ques.append(cur_rela)\n",
    "            logger.info(\"top3_ques rela: {}\".format(top3_ques))\n",
    "            relationship_replace_dict[i] = top3_ques[:7]\n",
    "    if len(relationship_replace_dict) > 5:\n",
    "        return None, updating_two_hop_rela_dict, None\n",
    "    elif len(relationship_replace_dict) >= 3:\n",
    "        for key in relationship_replace_dict:\n",
    "            relationship_replace_dict[key] = relationship_replace_dict[key][:4]\n",
    "    combinations = list(relationship_replace_dict.values())\n",
    "    all_iters = list(itertools.product(*combinations))\n",
    "    rela_index = list(relationship_replace_dict.keys())\n",
    "    logger.info(\"all_iters: {}\".format(all_iters))\n",
    "    for iters in all_iters:\n",
    "        expression_segment_copy = expression_segment.copy()\n",
    "        possible_entities_set = []\n",
    "        for i in range(len(iters)):\n",
    "            suffix = \"\"\n",
    "            for k in range(len(expression_segment[rela_index[i]].split(')')) - 1):\n",
    "                suffix = suffix + ')'\n",
    "            expression_segment_copy[rela_index[i]] = iters[i] + suffix\n",
    "            if iters[i] in relationship_to_enti:\n",
    "                possible_entities_set += relationship_to_enti[iters[i]]\n",
    "        if not possible_entities_set:\n",
    "            continue\n",
    "        enti_replace_dict = {}\n",
    "        for j, seg in enumerate(expression_segment):\n",
    "            processed_seg = seg.strip(')')\n",
    "            if '.' in seg and not seg.startswith('m.') and not seg.startswith('g.') and (\n",
    "                    expression_segment[j - 1].endswith(\"AND\") or expression_segment[j - 1].endswith(\"COUNT\") or\n",
    "                    expression_segment[j - 1].endswith(\"MAX\") or expression_segment[j - 1].endswith(\"MIN\")) and (\n",
    "            not any(ele.isupper() for ele in seg)):\n",
    "                tokenized_enti = [re.split('\\.|_', doc) for doc in possible_entities_set]\n",
    "                tokenized_query = re.split('\\.|_', processed_seg)\n",
    "                bm25 = BM25Okapi(tokenized_enti)\n",
    "                top3_ques = bm25.get_top_n(tokenized_query, possible_entities_set, n=3)\n",
    "                enti_replace_dict[j] = list(set(top3_ques))\n",
    "        combinations_enti = list(enti_replace_dict.values())\n",
    "        all_iters_enti = list(itertools.product(*combinations_enti))\n",
    "        enti_index = list(enti_replace_dict.keys())\n",
    "        for iter_ent in all_iters_enti:\n",
    "            for k in range(len(iter_ent)):\n",
    "                suffix = \"\"\n",
    "                for h in range(len(expression_segment[enti_index[k]].split(')')) - 1):\n",
    "                    suffix = suffix + ')'\n",
    "                expression_segment_copy[enti_index[k]] = iter_ent[k] + suffix\n",
    "            final = \" \".join(expression_segment_copy)\n",
    "            added = add_reverse(final)\n",
    "            for exp in added:\n",
    "                try:\n",
    "                    answer = generate_answer([exp])\n",
    "                except:\n",
    "                    answer = None\n",
    "                if answer is not None:\n",
    "                    return answer, updating_two_hop_rela_dict, exp\n",
    "    return None, updating_two_hop_rela_dict, None\n",
    "\n",
    "\n",
    "def generate_answer(list_exp):\n",
    "    for exp in list_exp:\n",
    "        try:\n",
    "            sparql = lisp_to_sparql(exp)\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            re = execute_query(sparql)\n",
    "        except:\n",
    "            continue\n",
    "        if re:\n",
    "            if re[0].isnumeric():\n",
    "                if re[0] == '0':\n",
    "                    continue\n",
    "                else:\n",
    "                    return re\n",
    "            else:\n",
    "                return re\n",
    "    return None\n",
    "\n",
    "\n",
    "def number_of_join(exp):\n",
    "    count = 0\n",
    "    seg_list = exp.split()\n",
    "    for seg in seg_list:\n",
    "        if \"JOIN\" in seg:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def process_file_codex_output(filename_before, filename_after):\n",
    "    codex_eps_dict_before = json.load(open(filename_before, 'r'), strict=False)\n",
    "    codex_eps_dict_after = json.load(open(filename_after, 'r'), strict=False)\n",
    "    for key in codex_eps_dict_after:\n",
    "        codex_eps_dict_before[key] = codex_eps_dict_after[key]\n",
    "    return codex_eps_dict_before\n",
    "\n",
    "def all_combiner_evaluation(data_batch, selected_quest_compare, selected_quest_compose, selected_quest,\n",
    "                            prompt_type, hsearcher, rela_corpus, relationships, temp, que_to_s_dict_train,\n",
    "                            question_to_mid_dict, api_key, LLM_engine, name_to_id_dict, bm25_all_fns, all_fns,\n",
    "                            relationship_to_enti, retrieval=False, corpus=None, nlp_model=None, bm25_train_full=None,\n",
    "                            retrieve_number=100):\n",
    "    correct = [0] * 6\n",
    "    total = [0] * 6\n",
    "    no_ans = [0] * 6\n",
    "    for data in data_batch:\n",
    "        logger.info(\"==========\")\n",
    "        logger.info(\"data[id]: {}\".format(data[\"id\"]))\n",
    "        logger.info(\"data[question]: {}\".format(data[\"question\"]))\n",
    "        logger.info(\"data[exp]: {}\".format(data[\"s_expression\"]))\n",
    "        label = []\n",
    "        for ans in data[\"answer\"]:\n",
    "            label.append(ans[\"answer_argument\"])\n",
    "        if not retrieval:\n",
    "            gene_type = type_generator(data[\"question\"], prompt_type, api_key, LLM_engine)\n",
    "            logger.info(\"gene_type: {}\".format(gene_type))\n",
    "        else:\n",
    "            gene_type = None\n",
    "\n",
    "        if gene_type == \"Comparison\":\n",
    "            gene_exps = ep_generator(data[\"question\"],\n",
    "                                     list(set(selected_quest_compare) | set(selected_quest)),\n",
    "                                     temp, que_to_s_dict_train, question_to_mid_dict, api_key, LLM_engine,\n",
    "                                     retrieval=retrieval, corpus=corpus, nlp_model=nlp_model,\n",
    "                                     bm25_train_full=bm25_train_full, retrieve_number=retrieve_number)\n",
    "        else:\n",
    "            gene_exps = ep_generator(data[\"question\"],\n",
    "                                     list(set(selected_quest_compose) | set(selected_quest)),\n",
    "                                     temp, que_to_s_dict_train, question_to_mid_dict, api_key, LLM_engine,\n",
    "                                     retrieval=retrieval, corpus=corpus, nlp_model=nlp_model,\n",
    "                                     bm25_train_full=bm25_train_full, retrieve_number=retrieve_number)\n",
    "        two_hop_rela_dict = {}\n",
    "        answer_candi = []\n",
    "        removed_none_candi = []\n",
    "        answer_to_grounded_dict = {}\n",
    "        logger.info(\"gene_exps: {}\".format(gene_exps))\n",
    "#         print(type(gene_exps))\n",
    "#         scouts = gene_exps[:6]\n",
    "        scouts = [gene_exps]\n",
    "#         print(scouts)\n",
    "        for idx, gene_exp in enumerate(scouts):\n",
    "            try:\n",
    "                logger.info(\"gene_exp: {}\".format(gene_exp))\n",
    "                join_num = number_of_join(gene_exp)\n",
    "                if join_num > 5:\n",
    "                    continue\n",
    "                if join_num > 3:\n",
    "                    top_mid = 5\n",
    "                else:\n",
    "                    top_mid = 15\n",
    "                found_names = find_friend_name(gene_exp, data[\"question\"])\n",
    "                logger.info(\"found_names:{}\".format(found_names))\n",
    "                found_mids = from_fn_to_id_set(found_names, data[\"question\"], name_to_id_dict, bm25_all_fns, all_fns)\n",
    "                logger.info(\"found_mids:{}\".format(found_mids))\n",
    "                found_mids = [mids[:top_mid] for mids in found_mids]\n",
    "                mid_combinations = list(itertools.product(*found_mids))\n",
    "                logger.info(\"all_iters: {}\".format(mid_combinations))\n",
    "                for mid_iters in mid_combinations:\n",
    "                    logger.info(\"mid_iters: {}\".format(mid_iters))\n",
    "                    replaced_exp = convz_fn_to_mids(gene_exp, found_names, mid_iters)\n",
    "                    logger.info(\"replaced_exp:{}\".format(replaced_exp))\n",
    "                    answer, two_hop_rela_dict, bounded_exp = bound_to_existed(data[\"question\"], replaced_exp, mid_iters,\n",
    "                                                                              two_hop_rela_dict, relationship_to_enti,\n",
    "                                                                              hsearcher, rela_corpus, relationships)\n",
    "                    \n",
    "                    logger.info(\"bounded_exp:{}\".format(bounded_exp))\n",
    "                    answer_candi.append(answer)\n",
    "                    if answer is not None:\n",
    "                        answer_to_grounded_dict[tuple(answer)] = bounded_exp\n",
    "                for ans in answer_candi:\n",
    "                    if ans != None:\n",
    "                        removed_none_candi.append(ans)\n",
    "                if not removed_none_candi:\n",
    "                    answer = None\n",
    "                else:\n",
    "                    count_dict = Counter([tuple(candi) for candi in removed_none_candi])\n",
    "                    logger.info(\"count_dict: {}\".format(count_dict))\n",
    "                    answer = max(count_dict, key=count_dict.get)\n",
    "            except:\n",
    "                if not removed_none_candi:\n",
    "                    answer = None\n",
    "                else:\n",
    "                    count_dict = Counter([tuple(candi) for candi in removed_none_candi])\n",
    "                    logger.info(\"count_dict: {}\".format(count_dict))\n",
    "                    answer = max(count_dict, key=count_dict.get)\n",
    "            answer_to_grounded_dict[None] = \"\"\n",
    "            logger.info(\"predicted_answer: {}\".format(answer))\n",
    "            logger.info(\"label: {}\".format(label))\n",
    "            if answer is None:\n",
    "                no_ans[idx] += 1\n",
    "            elif set(answer) == set(label):\n",
    "                correct[idx] += 1\n",
    "            total[idx] += 1\n",
    "            em_score = correct[idx] / total[idx]\n",
    "            logger.info(\"================================================================\")\n",
    "            logger.info(\"consistent candidates number: {}\".format(idx+1))\n",
    "            logger.info(\"em_score: {}\".format(em_score))\n",
    "            logger.info(\"correct: {}\".format(correct[idx]))\n",
    "            logger.info(\"total: {}\".format(total[idx]))\n",
    "            logger.info(\"no_ans: {}\".format(no_ans[idx]))\n",
    "            logger.info(\" \")\n",
    "            logger.info(\"================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(allow_abbrev=False)\n",
    "\n",
    "    parser.add_argument('--shot_num', type=int, metavar='N',\n",
    "                        default=40, help='the number of shots used in in-context demo')\n",
    "    parser.add_argument('--temperature', type=float, metavar='N',\n",
    "                        default=0.3, help='the temperature of LLM')\n",
    "    parser.add_argument('--api_key', type=str, metavar='N',\n",
    "                        default=None, help='the api key to access LLM')\n",
    "    parser.add_argument('--engine', type=str, metavar='N',\n",
    "                        default=\"code-davinci-002\", help='engine name of LLM')\n",
    "    parser.add_argument('--retrieval', action='store_true', help='whether to use retrieval-augmented KB-BINDER')\n",
    "    parser.add_argument('--train_data_path', type=str, metavar='N',\n",
    "                        default=\"data/GrailQA/grailqa_v1.0_train.json\", help='training data path')\n",
    "    parser.add_argument('--eva_data_path', type=str, metavar='N',\n",
    "                        default=\"data/GrailQA/grailqa_v1.0_dev.json\", help='evaluation data path')\n",
    "    parser.add_argument('--fb_roles_path', type=str, metavar='N',\n",
    "                        default=\"data/GrailQA/fb_roles\", help='freebase roles file path')\n",
    "    parser.add_argument('--surface_map_path', type=str, metavar='N',\n",
    "                        default=\"data/surface_map_file_freebase_complete_all_mention\", help='surface map file path')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78937a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(allow_abbrev=False)\n",
    "\n",
    "parser.add_argument('--shot_num', type=int, metavar='N',\n",
    "                    default=40, help='the number of shots used in in-context demo')\n",
    "parser.add_argument('--temperature', type=float, metavar='N',\n",
    "                    default=0.3, help='the temperature of LLM')\n",
    "parser.add_argument('--api_key', type=str, metavar='N',\n",
    "                    default=None, help='the api key to access LLM')\n",
    "parser.add_argument('--engine', type=str, metavar='N',\n",
    "                    default=\"code-davinci-002\", help='engine name of LLM')\n",
    "parser.add_argument('--retrieval', action='store_true', help='whether to use retrieval-augmented KB-BINDER')\n",
    "parser.add_argument('--train_data_path', type=str, metavar='N',\n",
    "                    default=\"data/GrailQA/grailqa_v1.0_train.json\", help='training data path')\n",
    "parser.add_argument('--eva_data_path', type=str, metavar='N',\n",
    "                    default=\"data/GrailQA/grailqa_v1.0_dev.json\", help='evaluation data path')\n",
    "parser.add_argument('--fb_roles_path', type=str, metavar='N',\n",
    "                    default=\"data/GrailQA/fb_roles\", help='freebase roles file path')\n",
    "parser.add_argument('--surface_map_path', type=str, metavar='N',\n",
    "                    default=\"data/surface_map_file_freebase_complete_all_mention\", help='surface map file path')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "bm25_searcher = LuceneSearcher('contriever_fb_relation/index_relation_fb')\n",
    "query_encoder = AutoQueryEncoder(encoder_dir='facebook/contriever', pooling='mean')\n",
    "contriever_searcher = FaissSearcher('contriever_fb_relation/freebase_contriever_index', query_encoder)\n",
    "hsearcher = HybridSearcher(contriever_searcher, bm25_searcher)\n",
    "rela_corpus = LuceneSearcher('contriever_fb_relation/index_relation_fb')\n",
    "dev_data = process_file(args.eva_data_path)\n",
    "train_data = process_file(args.train_data_path)\n",
    "que_to_s_dict_train = {data[\"question\"]: data[\"s_expression\"] for data in train_data}\n",
    "question_to_mid_dict = process_file_node(args.train_data_path)\n",
    "if not args.retrieval:\n",
    "    selected_quest_compose, selected_quest_compare, selected_quest = select_shot_prompt_train(train_data, args.shot_num)\n",
    "else:\n",
    "    selected_quest_compose = []\n",
    "    selected_quest_compare = []\n",
    "    selected_quest = []\n",
    "all_ques = selected_quest_compose + selected_quest_compare\n",
    "corpus = [data[\"question\"] for data in train_data]\n",
    "tokenized_train_data = []\n",
    "for doc in corpus:\n",
    "    nlp_doc = nlp(doc)\n",
    "    tokenized_train_data.append([token.lemma_ for token in nlp_doc])\n",
    "bm25_train_full = BM25Okapi(tokenized_train_data)\n",
    "if not args.retrieval:\n",
    "    prompt_type = ''\n",
    "    random.shuffle(all_ques)\n",
    "    for que in all_ques:\n",
    "        prompt_type = prompt_type + \"Question: \" + que + \"\\nType of the question: \"\n",
    "        if que in selected_quest_compose:\n",
    "            prompt_type += \"Composition\\n\"\n",
    "        else:\n",
    "            prompt_type += \"Comparison\\n\"\n",
    "else:\n",
    "    prompt_type = ''\n",
    "print(\"Begin to process fb_roles\")\n",
    "with open(args.fb_roles_path) as f:\n",
    "    lines = f.readlines()\n",
    "relationships = []\n",
    "entities_set = []\n",
    "relationship_to_enti = {}\n",
    "for line in lines:\n",
    "    info = line.split(\" \")\n",
    "    relationships.append(info[1])\n",
    "    entities_set.append(info[0])\n",
    "    entities_set.append(info[2])\n",
    "    relationship_to_enti[info[1]] = [info[0], info[2]]\n",
    "\n",
    "print(\"Procss fb_roles done!\")\n",
    "with open(args.surface_map_path) as f:\n",
    "    lines = f.readlines()\n",
    "name_to_id_dict = {}\n",
    "for line in lines:\n",
    "    info = line.split(\"\\t\")\n",
    "    name = info[0]\n",
    "    score = float(info[1])\n",
    "    mid = info[2].strip()\n",
    "    if name in name_to_id_dict:\n",
    "        name_to_id_dict[name][mid] = score\n",
    "    else:\n",
    "        name_to_id_dict[name] = {}\n",
    "        name_to_id_dict[name][mid] = score\n",
    "all_fns = list(name_to_id_dict.keys())\n",
    "tokenized_all_fns = [fn.split() for fn in all_fns]\n",
    "bm25_all_fns = BM25Okapi(tokenized_all_fns)\n",
    "print(\"Load Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "058e6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data_1 = dev_data[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c11b7603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 20:03:35 INFO     ==========\n",
      "2023-10-11 20:03:35 INFO     data[id]: 3202959008000\n",
      "2023-10-11 20:03:35 INFO     data[question]: what is the role of opera designer gig who designed the telephone / the medium?\n",
      "2023-10-11 20:03:35 INFO     data[exp]: (AND opera.opera_designer_role (JOIN (R opera.opera_designer_gig.design_role) (JOIN (R opera.opera_production.designers) m.0pm2fgf)))\n",
      "2023-10-11 20:03:38 INFO     gene_type: The type of the question is \"Composition\".\n",
      "2023-10-11 20:03:41 INFO     gene_exps: (AND opera.opera_designer (JOIN opera.opera_designer.role Opera Designer))\n",
      "2023-10-11 20:03:41 INFO     gene_exp: (AND opera.opera_designer (JOIN opera.opera_designer.role Opera Designer))\n",
      "2023-10-11 20:03:41 INFO     found_names:['opera designer']\n",
      "2023-10-11 20:03:41 INFO     found_mids:[['m.025dxyw', 'm.04nsplt']]\n",
      "2023-10-11 20:03:41 INFO     all_iters: [('m.025dxyw',), ('m.04nsplt',)]\n",
      "2023-10-11 20:03:41 INFO     mid_iters: ('m.025dxyw',)\n",
      "2023-10-11 20:03:41 INFO     replaced_exp:(AND opera.opera_designer (JOIN opera.opera_designer.role m.025dxyw))\n",
      "2023-10-11 20:03:41 INFO     before 2 hop rela\n",
      "2023-10-11 20:03:41 INFO     mids is ('m.025dxyw',)\n",
      "2023-10-11 20:03:41 INFO     mid is m.025dxyw\n",
      "2023-10-11 20:03:41 INFO     2\n",
      "2023-10-11 20:03:41 INFO     Exception is name 'exit' is not defined\n",
      "2023-10-11 20:03:41 INFO     predicted_answer: None\n",
      "2023-10-11 20:03:41 INFO     label: ['m.0b787yg']\n",
      "2023-10-11 20:03:41 INFO     ================================================================\n",
      "2023-10-11 20:03:41 INFO     consistent candidates number: 1\n",
      "2023-10-11 20:03:41 INFO     em_score: 0.0\n",
      "2023-10-11 20:03:41 INFO     correct: 0\n",
      "2023-10-11 20:03:41 INFO     total: 1\n",
      "2023-10-11 20:03:41 INFO     no_ans: 1\n",
      "2023-10-11 20:03:41 INFO      \n",
      "2023-10-11 20:03:41 INFO     ================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "            PREFIX : <http://rdf.freebase.com/ns/>\n",
      "            SELECT distinct ?x0 as ?r0 ?y as ?r1 WHERE {\n",
      "            ?x1 ?x0 :m.025dxyw. \n",
      "                ?x2 ?y ?x1 .\n",
      "                  FILTER regex(?x0, \"http://rdf.freebase.com/ns/\")\n",
      "                  FILTER regex(?y, \"http://rdf.freebase.com/ns/\")\n",
      "                  }\n",
      "                  \n"
     ]
    }
   ],
   "source": [
    "all_combiner_evaluation(dev_data_1, selected_quest_compose, selected_quest_compare, selected_quest, prompt_type,\n",
    "                            hsearcher, rela_corpus, relationships, args.temperature, que_to_s_dict_train,\n",
    "                            question_to_mid_dict, args.api_key, args.engine, name_to_id_dict, bm25_all_fns,\n",
    "                            all_fns, relationship_to_enti, retrieval=args.retrieval, corpus=corpus, nlp_model=nlp,\n",
    "                            bm25_train_full=bm25_train_full, retrieve_number=args.shot_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9fa9300e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['distinct', 'ordered', 'bindings'])\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import urllib\n",
    "# 已经知道 实体 mid 的情况下，获取实体的相关信息，例如获取friendly name 、 relation\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://localhost:3001/sparql\")\n",
    "query = \"\"\"\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "            PREFIX : <http://rdf.freebase.com/ns/>\n",
    "            SELECT distinct ?x0 as ?r0 ?y as ?r1 WHERE {\n",
    "            ?x1 ?x0 :m.025dxyw. \n",
    "                ?x2 ?y ?x1 .\n",
    "                  FILTER regex(?x0, \"http://rdf.freebase.com/ns/\")\n",
    "                  FILTER regex(?y, \"http://rdf.freebase.com/ns/\")\n",
    "                  }\n",
    "\"\"\"\n",
    "sparql.setQuery(query)\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.queryAndConvert()\n",
    "print(results[\"results\"].keys())\n",
    "\n",
    "print(len(results[\"results\"][\"bindings\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binder",
   "language": "python",
   "name": "binder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
